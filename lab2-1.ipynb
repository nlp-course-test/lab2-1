{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2-1 – Language modeling with n-grams\n",
    "\n",
    "$$\n",
    "   \\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "   \\newcommand{\\Prob}{{\\Pr}}\n",
    "   \\newcommand{\\given}{\\,|\\,}\n",
    "   \\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "   \\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "$$\n",
    "We turn from tasks that _classify_ texts – mapping texts into a finite set of classes – to tasks that _model_ texts by providing a full probability distribution over texts (or providing a similar scoring metric). Such language models attempt to answer the question \"How likely is a token sequence to be generated as an instance of the language?\".\n",
    "\n",
    "We'll start with n-gram language models. Given a token sequence $x_1, x_2, \\ldots, x_N$, its probability $\\Prob(x_1, x_2, \\ldots, x_N)$ can be calculated using the chain rule of probability:\n",
    "\n",
    "$$\\Prob(A, B \\given \\theta)= \\Prob(A \\given \\theta) \\cdot \\Prob(B \\given A, \\theta) $$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\\begin{align}\n",
    "\\Prob(x_1, x_2, \\ldots, x_N) & = \\Prob(x_1) \\cdot \\Prob(x_2, \\ldots, x_N \\given x_1) \\\\\n",
    "& = \\Prob(x_1) \\cdot \\Prob(x_2 \\given x_1) \\cdot \\Prob(x_3 \\ldots, x_N \\given x_1, x_2) \\\\\n",
    "& \\cdots \\\\\n",
    "& = \\prod_{i=1}^N \\Prob (x_i \\given x_1, \\cdots, x_{i-1}) \\\\\n",
    "& \\approx \\prod_{i=1}^N \\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})\\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the last step, we replace the probability $\\Prob (x_i \\given x_1, \\cdots, x_{i-1})$, which conditions $x_i$ on all of the preceding tokens, with $\\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})$, which conditions $x_i$ only on the $n-1$ preceding tokens. We call the $n-1$ preceding tokens ($x_{i-n+1}, \\cdots, x_{i-1}$) the _context_ and $x_i$ the target. Taken together, they form an $n$-gram, hence the term _$n$-gram model_.\n",
    "\n",
    "In this lab you'll work with $n$-gram models: generating them, sampling from them, and scoring held-out texts according to them. We'll find some problems with $n$-gram models as language models:\n",
    "\n",
    "1. They are profligate with memory.\n",
    "2. They are sensitive to very limited context.\n",
    "3. They don't generalize well across similar words.\n",
    "\n",
    "In the next lab, we'll explore neural models to address these failings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n",
    "\n",
    "* `itertools.product`\n",
    "* `random.random`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from sys import getsizeof\n",
    "import torchtext as tt\n",
    "\n",
    "# Otter grader which we use for grading does not support\n",
    "# !command, so we need to use shell(command) instead\n",
    "# to run shell commands\n",
    "def shell(str):\n",
    "    file = os.popen(str)\n",
    "    result = file.read()\n",
    "    print (result)\n",
    "    if file.close () is not None:\n",
    "        print ('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Some utilities to manipulate the corpus\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Strips #comments and empty lines from a string\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()              # trim whitespace\n",
    "        line = re.sub('#.*$', '', line)  # trim comments\n",
    "        if line != '':                   # drop blank lines\n",
    "            result.append(line)\n",
    "    return result\n",
    "\n",
    "def tokenize(lines):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        # tokenize\n",
    "        tokens = tt.data.get_tokenizer(\"basic_english\")(line)\n",
    "        # revert the speaker ID token\n",
    "        if tokens[0] == \"sam\":\n",
    "            tokens[0] = \"SAM:\"\n",
    "        elif tokens[0] == \"guy\":\n",
    "            tokens[0] = \"GUY:\"\n",
    "        else:\n",
    "            raise ValueError(\"format problem - bad speaker ID\")\n",
    "        # add a start of sentence token\n",
    "        result += [\"<s>\"] + tokens\n",
    "    return result\n",
    "                    \n",
    "def postprocess(tokens):\n",
    "    \"\"\"Converts `tokens` to a string with one sentence per line\"\"\"\n",
    "    return ' '.join(tokens)\\\n",
    "              .replace(\"<s> \", \"\\n\")\n",
    "\n",
    "# Read the GEaH data and preprocess into training and test streams of tokens\n",
    "geah_filename = (\"https://github.com/nlp-course/data/raw/master/Seuss/\"\n",
    "                 \"seuss - 1960 - green eggs and ham.txt\")\n",
    "shell(f'wget -nv -N -P data \"{geah_filename}\"')\n",
    "\n",
    "with open(\"data/seuss - 1960 - green eggs and ham.txt\", 'r') as fin:\n",
    "    lines = preprocess(fin.read())\n",
    "    train_lines = [lines[i] for i in range(0, len(lines)) if i % 12 != 0]\n",
    "    test_lines = [lines[i] for i in range(0, len(lines)) if i % 12 == 0]\n",
    "    train_tokens = tokenize(train_lines)\n",
    "    test_tokens = tokenize(test_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already loaded in the text of _Green Eggs and Ham_ for you and split it (about 90%/10%) into two token sequences, `train_tokens` and `test_tokens`. Here's a preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tokens[:50])\n",
    "print(postprocess(train_tokens[:50]))\n",
    "\n",
    "print(test_tokens[:50])\n",
    "print(postprocess(test_tokens[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the vocabulary from the training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vocabulary from dataset\n",
    "vocabulary = set(train_tokens)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating $n$-grams\n",
    "\n",
    "The _$n$-grams_ in a text are the contiguous subsequences of $n$ tokens. (We'll implement them as Python tuples.) In theory, any sequence of $n$ tokens is a potential $n$-gram type. Let's generate a list of all the possible $n$-gram types over a vocabulary. (Notice how the type/token distinction is useful for talking about $n$-grams, just as it is for words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def all_ngrams(vocabulary, n):\n",
    "    \"\"\"Returns a list of all `n`-long tuples of elements of the `vocabulary`.\n",
    "    \n",
    "    For instance,  \n",
    "        all_ngrams([\"one\", \"two\"], 3)\n",
    "        [('one', 'one', 'one'),\n",
    "         ('one', 'one', 'two'),\n",
    "         ('one', 'two', 'one'),\n",
    "         ('one', 'two', 'two'),\n",
    "         ('two', 'one', 'one'),\n",
    "         ('two', 'one', 'two'),\n",
    "         ('two', 'two', 'one'),\n",
    "         ('two', 'two', 'two')]\n",
    "    \"\"\"\n",
    "    \"your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "def all_ngrams(vocabulary, n):\n",
    "    \"\"\"Returns a list of all `n`-long tuples of elements of the `vocabulary`.\n",
    "    \n",
    "    For instance,  \n",
    "        >>> all_ngrams([\"one\", \"two\"], 3)\n",
    "        [('one', 'one', 'one'),\n",
    "         ('one', 'one', 'two'),\n",
    "         ('one', 'two', 'one'),\n",
    "         ('one', 'two', 'two'),\n",
    "         ('two', 'one', 'one'),\n",
    "         ('two', 'one', 'two'),\n",
    "         ('two', 'two', 'one'),\n",
    "         ('two', 'two', 'two')]\n",
    "    \"\"\"\n",
    "    return list(itertools.product(vocabulary, repeat=n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a list of all of the $n$-grams in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, n):\n",
    "    \"\"\"Returns a list of all `n`-grams in a list of `tokens`.\"\"\"\n",
    "    return [tuple(tokens[i : i + n])\n",
    "            for i in range(0, len(tokens) - n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting $n$-grams\n",
    "\n",
    "We'll conceptualize an $n$-gram as having two parts:\n",
    "\n",
    "* The _context_ is the first $n-1$ tokens in the $n$-gram.\n",
    "* The _target_ is the final token in the $n$-gram.\n",
    "\n",
    "An $n$-gram language model specifies a probability for each $n$-gram type. We'll implement the models as a 2-D dictionary, indexed first by context and then by target, providing the probability for the $n$-gram.\n",
    "\n",
    "We start by generating a similar data structure for counting up the $n$-grams in a token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_counts(vocabulary, tokens, n):\n",
    "    \"\"\"Returns a dictionary of counts of the `n`-grams in `tokens`, structured\n",
    "       with first index by (n-1)-gram context and second index by the final \n",
    "       target token.\n",
    "    \"\"\"\n",
    "    context_dict = defaultdict(lambda: defaultdict(int))\n",
    "    # zero all ngrams\n",
    "    for context in all_ngrams(vocabulary, n - 1):\n",
    "        for target in vocabulary:\n",
    "            context_dict[context][target] = 0\n",
    "    # add counts for attested ngrams\n",
    "    for ngram, count in Counter(ngrams(tokens, n)).items():\n",
    "        context_dict[ngram[:-1]][ngram[-1]] = count\n",
    "    return context_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ngram_counts` function to generate count data structures for unigrams, bigrams, and trigrams for the _Green Eggs and Ham_ training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "unigram_counts = \"your code here\"\n",
    "bigram_counts = \"your code here\"\n",
    "trigram_counts = \"your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "unigram_counts = ngram_counts(vocabulary, train_tokens, 1)\n",
    "bigram_counts = ngram_counts(vocabulary, train_tokens, 2)\n",
    "trigram_counts = ngram_counts(vocabulary, train_tokens, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your work by examining the total count of unigrams, bigrams, and trigrams. Do the numbers make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total counts of tokens, unigrams, bigrams, and trigrams\n",
    "token_count = len(train_tokens)\n",
    "unigram_count = sum(len(unigram_counts[cntxt].keys()) for cntxt in unigram_counts.keys())\n",
    "bigram_count = sum(len(bigram_counts[cntxt].keys()) for cntxt in bigram_counts.keys())\n",
    "trigram_count = sum(len(trigram_counts[cntxt].keys()) for cntxt in trigram_counts.keys())               \n",
    "\n",
    "# Report on the totals\n",
    "print(f\"Tokens:   {token_count:6}\\n\"\n",
    "      f\"Unigrams: {unigram_count:6}\\n\"\n",
    "      f\"Bigrams:  {bigram_count:6}\\n\"\n",
    "      f\"Trigrams: {trigram_count:6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating $n$-gram probabilities\n",
    "\n",
    "We can convert the counts into a probability model by _normalizing_ the counts. Given an $n$-gram type $x_1, x_2, \\ldots, x_n$, instead of storing the count $\\cnt{x_1, x_2, \\ldots, x_n}$, we store an estimate of the probability \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\Pr(x_n \\given x_1, x_2, \\ldots, x_{n-1})\n",
    "  & \\approx \\frac{\\cnt{x_1, x_2, \\ldots, x_n}}{\\cnt{x_1, x_2, \\ldots, x_{n-1}}} \\\\\n",
    "  & = \\frac{\\cnt{x_1, x_2, \\ldots, x_n}}{\\sum_{x'} \\cnt{x_1, x_2, \\ldots, x_{n-1}, x'}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "that is, the ratio of the count of the $n$-gram and the sum of the counts of all $n$-grams with the same context. Fortunately, all of those counts are already stored in the count data structures we've already built. \n",
    "\n",
    "Write a function that takes an $n$-gram count data structure and returns an $n$-gram probability data structure. As with the counts, the probabilities should be stored indexed first by context and then by target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def ngram_model(ngram_counts):\n",
    "    \"\"\"Returns an n-gram probability model calculated by normalizing the \n",
    "       provided `ngram-counts` dictionary\n",
    "    \"\"\"\n",
    "    \"your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "def ngram_model(ngram_counts):\n",
    "    \"\"\"Returns an n-gram probability model calculated by normalizing the \n",
    "       provided `ngram-counts` dictionary\n",
    "    \"\"\"\n",
    "    probs = defaultdict(lambda: defaultdict(int))\n",
    "    for cntxt, distrib in ngram_counts.items():\n",
    "        total_count = sum(distrib.values())\n",
    "        for token in distrib.keys():\n",
    "            probs[cntxt][token] = distrib[token] / total_count if total_count > 0 else 0.0\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build some $n$-gram models – unigram, bigram, and trigram – based on the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model = ngram_model(unigram_counts)\n",
    "bigram_model = ngram_model(bigram_counts)\n",
    "trigram_model = ngram_model(trigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, we aren't too concerned in this course about matters of time or space efficiency, though these are crucial issues in the engineering of NLP systems. But the size of $n$-gram models merits consideration, looking especially at their size as $n$ grows. We can use Python's `sys.getsizeof` function to get a rough sense of the size of the models we've been working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokens:   {getsizeof(train_tokens):6}\\n\"\n",
    "      f\"Unigrams: {getsizeof(unigram_model):6}\\n\"\n",
    "      f\"Bigrams:  {getsizeof(bigram_model):6}\\n\"\n",
    "      f\"Trigrams: {getsizeof(trigram_model):6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these sizes tell you about the memory usage of $n$-gram models? With a larger vocabulary of, say, 10,000 words, would it be practical to run, say, 5-gram models on your laptop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "open_response_1 = \"your answer here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "open_response_1 = \"\"\"\n",
    "    The number of n-grams grows exponentially in n: O(V^n) where V is the \n",
    "    vocabulary size. That means that, without sparse representations or \n",
    "    other tricks, storing all of the parameters of a language model quickly\n",
    "    becomes prohibitive. For instance, (10^4)^5 = 10^20, so storing a 5-gram \n",
    "    model with a vocabulary size of 10,000 would require something like 10^18\n",
    "    terabytes. That's one honking large laptop.\n",
    "    \n",
    "    The smoothing section below may provide some considerations that indicate\n",
    "    why sparse representations are tricky to implement.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from an $n$-gram model\n",
    "\n",
    "We have cleverly constructed the models to index by context. This allows us to sample a word given its context. For instance, in the trigram context `(\"<s>\", \"SAM:\")`, the following probability distribution captures which words can come next and with what probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model[(\"<s>\", \"SAM:\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample a single token according to this probability distribution. Here's one way to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, context):\n",
    "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n",
    "    distribution = model[context]\n",
    "    prob_remaining = random.random()\n",
    "    for token, prob in distribution.items():\n",
    "        if prob_remaining < prob:\n",
    "            return token\n",
    "        else:\n",
    "            prob_remaining -= prob\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the sampling to a sequence of words by updating the context as we sample each word.\n",
    "\n",
    "Define a function `sample_sequence` that performs this sampling of a sequence. It's given a model and a starting context and begins by sampling the first token based on the starting context, then updates the starting context to reflect the word just sampled, repeating the process until a specified number of tokens have been sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def sample_sequence(model, start_context, count=100):\n",
    "    \"\"\"Returns a sequence of tokens of length `count` sampled successively\n",
    "       from the `model` startring with the `start_context`\n",
    "    \"\"\"\n",
    "    \"your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "def sample_sequence(model, start_context, count=100):\n",
    "    \"\"\"Returns a sequence of tokens of length `count` sampled successively\n",
    "       from the `model` startring with the `start_context`\n",
    "    \"\"\"\n",
    "    context = list(start_context)\n",
    "    result = list(start_context)\n",
    "    for i in range(0, count):\n",
    "        next = sample(model, tuple(context))\n",
    "        result.append(next)\n",
    "        context = (context + [next])[1:]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(postprocess(sample_sequence(unigram_model, ())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(postprocess(sample_sequence(bigram_model, (\"<s>\",))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(postprocess(sample_sequence(trigram_model, (\"<s>\", \"SAM:\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating text according to an $n$-gram model\n",
    "\n",
    "### The probability metric\n",
    "\n",
    "The main point of a language model is to assign probabilities (or similar scores) to texts. For $n$-gram models, that's done according to Equation (1) at the start of the lab. Let's implement that. We define a function `probability` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the probability of the token sequence  according to the model. It merely multiplies all of the $n$-gram probabilities for all of the $n$-grams in the token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(tokens, model, n):\n",
    "    \"\"\"Returns the probability of a sequence of `tokens` according to an\n",
    "       `n`-gram `model`\n",
    "    \"\"\"\n",
    "    score = 1.0\n",
    "    context = tokens[0:n-1]\n",
    "    for token in tokens[n-1:]:\n",
    "        prob = model[tuple(context)][token]\n",
    "        score *= prob\n",
    "        context = (context + [token])[1:]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test it on the test text that we held out from the training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test probability - unigram: {probability(test_tokens, unigram_model, 1):6e}\\n\"\n",
    "      f\"Test probability -  bigram: {probability(test_tokens, bigram_model, 2):6e}\\n\"\n",
    "      f\"Test probability - trigram: {probability(test_tokens, trigram_model, 3):6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The negative log probability metric\n",
    "\n",
    "Yikes, those probabilities are _really small_. Multiplying all those small numbers is likely to lead to underflow. \n",
    "\n",
    "To solve the underflow problem, we'll do our usual trick of using negative log probabilities \n",
    "\n",
    "$$ - \\log_2 \\left(\\prod_{i=1}^N \\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})\\right)$$\n",
    "\n",
    "instead of probabilities.\n",
    "\n",
    "Define a function `neglogprob` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the negative log probability of the token sequence according to the model, calculating it in such a way as to avoid underflow. (You'll want to simplify the formula above before implementing it.)\n",
    "\n",
    "> Be careful when confronting zero probabilities. Taking `-math.log2(0)` raises a \"Math domain error\". Instead, you should use `math.inf` (Python's representation of infinity) as the value for the negative log of zero. This accords with our understanding that an impossible event would require infinite bits to specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def neglogprob(tokens, model, n):\n",
    "    \"\"\"Returns the negative log probability of a sequence of `tokens` \n",
    "       according to an `n`-gram `model`\n",
    "    \"\"\"\n",
    "    \"your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "def neglogprob(tokens, model, n):\n",
    "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n",
    "       according to an `n`-gram `model`\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    context = tokens[0:n-1]\n",
    "    for token in tokens[n-1:]:\n",
    "        prob = model[tuple(context)][token]\n",
    "        score += - math.log2(prob) if prob > 0 else math.inf\n",
    "        context = (context + [token])[1:]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the negative log probabilities of the test text using the different models and report on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_test_nlp = neglogprob(test_tokens, unigram_model, 1)\n",
    "bigram_test_nlp = neglogprob(test_tokens, bigram_model, 2)\n",
    "trigram_test_nlp = neglogprob(test_tokens, trigram_model, 3)\n",
    "\n",
    "print(f\"Test neglogprob - unigram: {unigram_test_nlp:6f}\\n\"\n",
    "      f\"Test neglogprob -  bigram: {bigram_test_nlp:6f}\\n\"\n",
    "      f\"Test neglogprob - trigram: {trigram_test_nlp:6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, those numbers seem more manageable. We can even convert the neglogprobs back into probabilities as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test neglogprob - unigram: {2 ** (-unigram_test_nlp):6e}\\n\"\n",
    "      f\"Test neglogprob -  bigram: {2 ** (-bigram_test_nlp):6e}\\n\"\n",
    "      f\"Test neglogprob - trigram: {2 ** (-trigram_test_nlp):6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the bigram model assign a lower neglogprob (that is, a higher probability) to the test text than the unigram model? Why does the trigram model assign a higher neglogprob (lower probability) to the test text than the other models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "open_response_1 = \"your answer here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "open_response_1 = \"\"\"\n",
    "    The bigram model models the probability more accurately because \n",
    "    of its larger context. But then why isn't the trigram model \n",
    "    better still? Because some of the probabilities in the trigram\n",
    "    model are zero. It has overfit the training data, assuming that \n",
    "    since certain trigrams did not occur in the training data, they\n",
    "    cannot occur at all! The trigram model is in desperate need of\n",
    "    smoothing.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perplexity metric\n",
    "\n",
    "Another metric that is commonly used is _perplexity_. Jurafsky and Martin give a definition for perplexity as the \"inverse probability of the test set normalized by the number of words\":\n",
    "\n",
    "$$ PP(x_1, x_2, \\ldots, x_N) = \n",
    "     \\sqrt[N]{\\frac{1}{\\prod_{i=1}^N \\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})}}\n",
    "$$\n",
    "\n",
    "Define a function `perplexity` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the perplexity of the token sequence according to the model, calculating it in such a way as to avoid underflow. (By now you're smart enough to realize that you'll want to carry out most of that calculation inside a $\\log$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def perplexity(tokens, model, n):\n",
    "    \"\"\"Returns the perplexity of a sequence of `tokens` according to an\n",
    "       `n`-gram `model`\n",
    "    \"\"\"\n",
    "    \"your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution\n",
    "def perplexity(tokens, model, n):\n",
    "    \"\"\"Returns the perplexity of a sequence of `tokens` according to an\n",
    "       `n`-gram `model`\n",
    "    \"\"\"\n",
    "    return 2 ** (neglogprob(tokens, model, n) / (len(tokens) - n + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the perplexity of the test sample according to each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model, 1):6e}\\n\"\n",
    "      f\"Test perplexity -  bigram: {perplexity(test_tokens, bigram_model, 2):6e}\\n\"\n",
    "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model, 3):6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perplexity value of $P$ can be interpreted as a measure of a model's average uncertainty in selecting each word equivalent to selecting among $P$ equiprobable words on average. The bigram model gives a perplexity of less than 3, indicating that at each word in the sentence, the model is acting as if selecting among (slightly less than) three equiprobable words.\n",
    "\n",
    "For comparison, state of the art $n$-gram language models for more representative English text achieve perplexities of about 250."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus section: Smoothing $n$-gram language models\n",
    "\n",
    "> **This section is more open-ended in nature and need only be turned in for the consensus submission of the lab.**\n",
    "\n",
    "The models we've been using have lots of zero-probability $n$-grams. Essentially any $n$-gram that doesn't appear on the training text is imputed a probability of zero, which means that any sentence that contains that $n$-gram will also be given a zero probability. Clearly this is not an accurate estimate.\n",
    "\n",
    "There are many ways to _smooth_ $n$-gram models, just as you smoothed classification models in earlier labs. The simplest is probably add-$\\delta$ smoothing. \n",
    "\n",
    "$$ \\Prob(x_i \\given x_1 \\ldots x_{i-1})\n",
    "  \\approx \\frac{\\cnt{x_1, x_2, \\ldots, x_n} + \\delta}{\\cnt{x_1, x_2, \\ldots, x_{n-1}} + \\delta \\cdot |V|}\n",
    "$$\n",
    "\n",
    "Another useful method is to interpolate multiple $n$-gram models, for instance, estimating probabilities as an interpolation of trigram, bigram, and unigram models.\n",
    "\n",
    "$$ \\Prob(x_i \\given x_1 \\ldots x_{i-1}) \\approx\n",
    "     \\lambda_2 \\Prob(x_i \\given x_{i-2}, x_{i-1}) \n",
    "     + \\lambda_1 \\Prob(x_i \\given x_{i-1}) \n",
    "     + (1 - \\lambda_1 - \\lambda_2) \\Prob(x_i)\n",
    "$$\n",
    "\n",
    "Finally, a method called _backoff_ uses higher-order $n$-gram probabilities where available, \"backing off\" to lower order where necessary.\n",
    "\n",
    "$$ \\Prob(x_i \\given x_1 \\ldots x_{i-1}) \\approx \\left\\{\n",
    "\\begin{align}\n",
    "     &\\Prob(x_i \\given x_{i-2}, x_{i-1}) & \\mbox{if $\\Prob(x_i \\given x_{i-2}, x_{i-1}) > 0$}\\\\\n",
    "     &\\Prob(x_i \\given x_{i-1}) & \\mbox{if $\\Prob(x_i \\given x_{i-2}, x_{i-1}) = 0$ and $\\Prob(x_i \\given x_{i-1}) > 0$}\\\\\n",
    "     &\\Prob(x_i) & \\mbox{otherwise}\n",
    "\\end{align} \\right.\n",
    "$$\n",
    "\n",
    "Define a function `ngram_model_smoothed`, like the `ngram_model` function from above, but implementing one of these smoothing methods. Compare its perplexity on some sample text to the unsmoothed model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def ngram_model_smoothed(ngram_counts):\n",
    "    \"\"\"Returns an n-gram probability model calculated by normalizing the \n",
    "       provided `ngram-counts` dictionary\n",
    "    \"\"\"\n",
    "    \"your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution - using add-delta smoothing\n",
    "def ngram_model_smoothed(ngram_counts, delta=0):\n",
    "    \"\"\"Returns an n-gram probability model calculated by normalizing the \n",
    "       provided `ngram-counts` dictionary, with add-delta smoothing\n",
    "    \"\"\"\n",
    "    vocab_size = len(list(ngram_counts.items())[0][1].keys())\n",
    "    probs = defaultdict(lambda: defaultdict(int))\n",
    "    for cntxt, distrib in ngram_counts.items():\n",
    "        total_count = sum(distrib.values())\n",
    "        for token in distrib.keys():\n",
    "            probs[cntxt][token] = (distrib[token] + delta) / (total_count + vocab_size * delta)\n",
    "            if probs[cntxt][token] == 0:\n",
    "                print(\"{context} {token} prob is zero\")\n",
    "    return probs\n",
    "\n",
    "# We define a smoothed trigram model\n",
    "\n",
    "trigram_smoothed = ngram_model_smoothed(trigram_counts, delta=0.001)\n",
    "\n",
    "# and test it by computing its perplexity compared to the unsmoothed\n",
    "# trigram and bigram models\n",
    "\n",
    "print(f\"Perplexity of smoothed trigram:   {perplexity(test_tokens, trigram_smoothed, 3):.6f}\\n\"\n",
    "      f\"Perplexity of unsmoothed trigram: {perplexity(test_tokens, trigram_model, 3):.6f}\\n\"\n",
    "      f\"Perplexity of unsmoothed bigram:  {perplexity(test_tokens, bigram_model, 2):.6f}\")\n",
    "\n",
    "# Notice that the smoothed trigram perplexity is not only better than the \n",
    "# unsmoothed trigram model, but better than the unsmoothed bigram model as well.\n",
    "\n",
    "# Here's a sample generated from the smoothed trigram model\n",
    "\n",
    "print(postprocess(sample_sequence(trigram_smoothed, (\"<s>\", \"SAM:\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Lab 2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(\"lab2-1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
